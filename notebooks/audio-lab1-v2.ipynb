{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":847347,"sourceType":"datasetVersion","datasetId":212383}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.412265Z","iopub.execute_input":"2025-02-18T22:20:44.412727Z","iopub.status.idle":"2025-02-18T22:20:44.418570Z","shell.execute_reply.started":"2025-02-18T22:20:44.412680Z","shell.execute_reply":"2025-02-18T22:20:44.417113Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# !pip install pyloudnorm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.420664Z","iopub.execute_input":"2025-02-18T22:20:44.421148Z","iopub.status.idle":"2025-02-18T22:20:44.442475Z","shell.execute_reply.started":"2025-02-18T22:20:44.421102Z","shell.execute_reply":"2025-02-18T22:20:44.441230Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import os\nimport IPython.display as ipd\nimport librosa\nfrom glob import glob\nimport soundfile as sf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import signal, linalg\nimport sklearn\nfrom ipywidgets import interact\nimport urllib\n\nimport torch\nfrom pprint import pprint\n\nfrom sklearn import preprocessing\nfrom sklearn import cluster\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom tqdm import tqdm\nimport numpy as np\nimport pyloudnorm as pyln\nimport soundfile as sf\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.444960Z","iopub.execute_input":"2025-02-18T22:20:44.445340Z","iopub.status.idle":"2025-02-18T22:20:44.458348Z","shell.execute_reply.started":"2025-02-18T22:20:44.445311Z","shell.execute_reply":"2025-02-18T22:20:44.457206Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def load_vad_model():\n    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n    speech_timestamps = utils[0]\n    read_audio = utils[2]\n    \n    return speech_timestamps, model, read_audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.459853Z","iopub.execute_input":"2025-02-18T22:20:44.460250Z","iopub.status.idle":"2025-02-18T22:20:44.478955Z","shell.execute_reply.started":"2025-02-18T22:20:44.460223Z","shell.execute_reply":"2025-02-18T22:20:44.477528Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import pyloudnorm as pyln\nimport numpy as np\n\ndef loudness_normalization(audio, target_lufs=-23.0):\n    meter = pyln.Meter(16000)\n    loudness = meter.integrated_loudness(audio)\n    gain = target_lufs - loudness\n    \n    normalized_audio = audio * (10**(gain / 20))\n    \n    return normalized_audio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.480385Z","iopub.execute_input":"2025-02-18T22:20:44.480836Z","iopub.status.idle":"2025-02-18T22:20:44.504174Z","shell.execute_reply.started":"2025-02-18T22:20:44.480792Z","shell.execute_reply":"2025-02-18T22:20:44.502795Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def apply_filters(audio, sr):\n    hp_cutoff = 80 \n    b, a = signal.butter(4, hp_cutoff / (sr / 2), btype='highpass')\n    audio = signal.filtfilt(b, a, audio)\n\n    lp_cutoff = 7000\n    b, a = signal.butter(4, lp_cutoff / (sr / 2), btype='lowpass')\n    audio = signal.filtfilt(b, a, audio)\n\n    return audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.505666Z","iopub.execute_input":"2025-02-18T22:20:44.506152Z","iopub.status.idle":"2025-02-18T22:20:44.526211Z","shell.execute_reply.started":"2025-02-18T22:20:44.506105Z","shell.execute_reply":"2025-02-18T22:20:44.524636Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def apply_silero_vad(audio, speech_timestamps, model):\n    audio_tensor = torch.tensor(audio, dtype=torch.float32)\n    \n    speech_segments = get_speech_timestamps(\n        audio_tensor,\n        model,\n        return_seconds=False\n    )\n    return speech_segments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:20:44.527732Z","iopub.execute_input":"2025-02-18T22:20:44.528073Z","iopub.status.idle":"2025-02-18T22:20:44.547072Z","shell.execute_reply.started":"2025-02-18T22:20:44.528033Z","shell.execute_reply":"2025-02-18T22:20:44.545711Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def extract_features(audio, sr):\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n\n    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n\n    zcr = librosa.feature.zero_crossing_rate(y=audio)\n\n    spectral_flux = librosa.onset.onset_strength(y=audio, sr=sr)\n    spectral_flux = spectral_flux.reshape(1, -1)\n\n    ste = librosa.feature.rms(y=audio)\n\n    # spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, roll_percent=0.85)\n\n    # spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n\n    # spectral_flatness = librosa.feature.spectral_flatness(y=audio)\n    \n    # features_list = [mfccs, spectral_centroid, zcr, spectral_flux, ste, spectral_rolloff, spectral_bandwidth, spectral_flatness]\n    features_list = [mfccs, spectral_centroid, zcr, spectral_flux, ste]\n\n    num_frames = min(f.shape[1] for f in features_list)\n    \n    features = np.vstack([f[:, :num_frames] for f in features_list]).T\n\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:24:54.435764Z","iopub.execute_input":"2025-02-18T22:24:54.436180Z","iopub.status.idle":"2025-02-18T22:24:54.444025Z","shell.execute_reply.started":"2025-02-18T22:24:54.436152Z","shell.execute_reply":"2025-02-18T22:24:54.442613Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def normalize_features(features, method=\"standard\"):\n    if method == \"standard\":\n        scaler = StandardScaler()\n    elif method == \"minmax\":\n        scaler = MinMaxScaler()\n\n    return scaler.fit_transform(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:24:25.042531Z","iopub.execute_input":"2025-02-18T22:24:25.042870Z","iopub.status.idle":"2025-02-18T22:24:25.048298Z","shell.execute_reply.started":"2025-02-18T22:24:25.042845Z","shell.execute_reply":"2025-02-18T22:24:25.046963Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def process_audio_file(audio_path, speech_timestamps, model, sr=16000, chunk_duration=0.025):\n    audio, sr = librosa.load(audio_path, sr=sr, mono=True)\n\n    audio = apply_filters(audio, sr)\n    audio = loudness_normalization(audio)\n    \n    speech_segments = apply_silero_vad(audio, get_speech_timestamps, model)\n\n    features = extract_features(audio, sr)\n    \n    labels = []\n    for i in range(features.shape[0]):\n        chunk_start = int(i * chunk_duration * sr)\n        chunk_end = chunk_start + int(chunk_duration * sr)\n\n        is_speech = any(s['start'] <= chunk_start and s['end'] >= chunk_end for s in speech_segments)\n        labels.append(1 if is_speech else 0)\n\n    if len(labels) != features.shape[0]:\n        if len(labels) > features.shape[0]:\n            labels = labels[:features.shape[0]]\n        else:\n            labels.extend([0] * (features.shape[0] - len(labels)))\n\n    features = normalize_features(features, method=\"standard\")\n\n    num_features = features.shape[1]\n    feature_names = [f\"feature_{i}\" for i in range(num_features)]\n\n    df = pd.DataFrame(features, columns=feature_names)\n    df[\"vad_label\"] = labels\n    df[\"start_time\"] = [(i * chunk_duration) for i in range(len(labels))]\n    df[\"end_time\"] = [((i + 1) * chunk_duration) for i in range(len(labels))]\n    df[\"file_path\"] = audio_path\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:24:57.603666Z","iopub.execute_input":"2025-02-18T22:24:57.604107Z","iopub.status.idle":"2025-02-18T22:24:57.614664Z","shell.execute_reply.started":"2025-02-18T22:24:57.604076Z","shell.execute_reply":"2025-02-18T22:24:57.613316Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def process_directory(input_dir, output_csv, get_speech_timestamps, model):\n\n    all_data = []  \n    processed_count = 0\n    error_count = 0\n    \n    for root, dirs, files in os.walk(input_dir):\n        for file in tqdm(files):\n            if file.endswith(\".wav\"): \n                audio_path = os.path.join(root, file)\n                \n                try:\n                    df = process_audio_file(audio_path, get_speech_timestamps, model)\n                    all_data.append(df)\n                    processed_count += 1\n                    \n                    if processed_count % 100 == 0:\n                        intermediate_df = pd.concat(all_data, ignore_index=True)\n                        intermediate_df.to_csv(f\"{output_csv}.intermediate_{processed_count}\", index=False)\n                            \n                except Exception as e:\n                    error_count += 1\n                    print(f\"Error processing {audio_path}: {str(e)}\")\n                    continue\n\n    if all_data:\n        combined_df = pd.concat(all_data, ignore_index=True)\n        combined_df.to_csv(output_csv, index=False)\n\n\ninput_directory = \"/kaggle/input/english-multispeaker-corpus-for-voice-cloning/VCTK-Corpus/VCTK-Corpus/wav48/p225\"\noutput_csv_path = \"output_features.csv\"\n\ntorch.set_default_tensor_type(torch.FloatTensor)\n\nget_speech_timestamps, model, read_audio = load_vad_model()\n\nprocess_directory(input_directory, output_csv_path, get_speech_timestamps, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:25:00.333614Z","iopub.execute_input":"2025-02-18T22:25:00.333981Z","iopub.status.idle":"2025-02-18T22:25:33.668547Z","shell.execute_reply.started":"2025-02-18T22:25:00.333955Z","shell.execute_reply":"2025-02-18T22:25:33.667484Z"}},"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n100%|██████████| 231/231 [00:32<00:00,  7.22it/s]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"def align_cluster_labels(cluster_labels, vad_labels):\n    if set(np.unique(cluster_labels)).issubset({0, 1, -1}):\n        non_noise_mask = cluster_labels != -1\n        filtered_clusters = cluster_labels[non_noise_mask]\n        filtered_vad = vad_labels[non_noise_mask]\n        \n        if len(filtered_clusters) == 0:\n            return np.zeros_like(cluster_labels)\n            \n        contingency = pd.crosstab(filtered_clusters, filtered_vad)\n        \n        if 0 in contingency.index and 1 in contingency.index:\n            if contingency.loc[0, 1] > contingency.loc[1, 1]:\n               \n                mapping = {0: 1, 1: 0, -1: 0} \n            else:\n                mapping = {0: 0, 1: 1, -1: 0}  \n        else:\n            only_label = list(contingency.index)[0]\n            if contingency.loc[only_label, 1] > contingency.loc[only_label, 0]:\n                mapping = {only_label: 1, -1: 0}\n            else:\n                mapping = {only_label: 0, -1: 0}\n    \n    labels = np.array([mapping.get(l, 0) for l in cluster_labels])\n    return labels\n\ndef evaluate_clustering(vad_labels, cluster_labels):\n    aligned_labels = align_cluster_labels(cluster_labels, vad_labels)\n    \n    accuracy = accuracy_score(vad_labels, aligned_labels)\n    precision = precision_score(vad_labels, aligned_labels, zero_division=0)\n    recall = recall_score(vad_labels, aligned_labels, zero_division=0)\n    f1 = f1_score(vad_labels, aligned_labels, zero_division=0)\n    conf_matrix = confusion_matrix(vad_labels, aligned_labels)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'confusion_matrix': conf_matrix,\n        'aligned_labels': aligned_labels\n    }\n\ndef main():\n    csv_file = \"/kaggle/working/output_features.csv\"\n    df = pd.read_csv(csv_file)\n    \n    feature_columns = [col for col in df.columns if col not in ['vad_label', 'start_time', 'end_time', 'file_path']]\n    \n    features = df[feature_columns].values\n    vad_labels = df['vad_label'].values\n    \n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features)\n    \n    kmeans = KMeans(n_clusters=2, random_state=42)\n    kmeans_labels = kmeans.fit_predict(features_scaled)\n    \n    results = evaluate_clustering(vad_labels, kmeans_labels)\n    \n    print(\"Evaluation Results for KMeans Clustering:\")\n    print(f\"Accuracy: {results['accuracy']:.4f}\")\n    print(f\"Precision: {results['precision']:.4f}\")\n    print(f\"Recall: {results['recall']:.4f}\")\n    print(f\"F1 Score: {results['f1_score']:.4f}\")\n    \n\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:25:36.706944Z","iopub.execute_input":"2025-02-18T22:25:36.707388Z","iopub.status.idle":"2025-02-18T22:25:37.333605Z","shell.execute_reply.started":"2025-02-18T22:25:36.707356Z","shell.execute_reply":"2025-02-18T22:25:37.332396Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results for KMeans Clustering:\nAccuracy: 0.6996\nPrecision: 0.7137\nRecall: 0.6344\nF1 Score: 0.6717\n","output_type":"stream"}],"execution_count":52}]}