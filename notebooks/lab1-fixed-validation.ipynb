{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:26:13.874479Z",
     "iopub.status.busy": "2025-02-24T10:26:13.874069Z",
     "iopub.status.idle": "2025-02-24T10:26:13.887811Z",
     "shell.execute_reply": "2025-02-24T10:26:13.886760Z",
     "shell.execute_reply.started": "2025-02-24T10:26:13.874439Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T15:18:39.104480Z",
     "start_time": "2025-03-02T15:18:39.099306Z"
    }
   },
   "source": [
    "import os\n",
    "import librosa\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import wiener"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:26:13.889632Z",
     "iopub.status.busy": "2025-02-24T10:26:13.889209Z",
     "iopub.status.idle": "2025-02-24T10:26:13.913060Z",
     "shell.execute_reply": "2025-02-24T10:26:13.911700Z",
     "shell.execute_reply.started": "2025-02-24T10:26:13.889576Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T15:18:40.009962Z",
     "start_time": "2025-03-02T15:18:39.984285Z"
    }
   },
   "source": [
    "def apply_filters(audio, sr):\n",
    "    '''\n",
    "    apply high-pass filter to filter out non-speech noise < 60Hz\n",
    "    '''\n",
    "    hp_cutoff = 80\n",
    "    Wn_hp = hp_cutoff / (sr / 2)\n",
    "    \n",
    "    b, a = signal.butter(4, Wn_hp, btype='highpass')\n",
    "    audio = signal.filtfilt(b, a, audio)\n",
    "\n",
    "    return audio\n",
    "    \n",
    "def loudness_normalization(audio):\n",
    "    \n",
    "    return librosa.util.normalize(audio)\n",
    "\n",
    "def apply_wiener_filter(y, win_length=7):\n",
    "    '''Noise filter'''\n",
    "    filtered_audio = wiener(y, mysize=win_length)\n",
    "    return filtered_audio\n",
    "\n",
    "def extract_features(audio, sr, chunk_start, chunk_end, context_size=5):\n",
    "    '''\n",
    "    Extracting features in current chunk and in context window of n chunks\n",
    "    '''\n",
    "    current_chunk = audio[chunk_start:chunk_end]\n",
    "    \n",
    "    context_samples = int(context_size * (chunk_end - chunk_start))\n",
    "    left_start = max(0, chunk_start - context_samples)\n",
    "    right_end = min(len(audio), chunk_end + context_samples)\n",
    "    \n",
    "    left_context = audio[left_start:chunk_start]\n",
    "    right_context = audio[chunk_end:right_end]\n",
    "    \n",
    "    full_context = np.concatenate([left_context, current_chunk, right_context])\n",
    "    \n",
    "    n_fft = min(512, len(current_chunk))\n",
    "    hop_length = n_fft // 4\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        y=current_chunk, \n",
    "        sr=sr, \n",
    "        n_mfcc=13,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    \n",
    "    mfccs_with_context = librosa.feature.mfcc(\n",
    "        y=full_context, \n",
    "        sr=sr, \n",
    "        n_mfcc=13,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    # delta features need context window, they track how the mfccs change over time \n",
    "    mfcc_delta = librosa.feature.delta(mfccs_with_context)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfccs_with_context, order=2)\n",
    "    \n",
    "    if mfccs.shape[1] == 0:\n",
    "        mfccs = np.zeros((13, 1))\n",
    "    if mfcc_delta.shape[1] == 0:\n",
    "        mfcc_delta = np.zeros((13, 1))\n",
    "    if mfcc_delta2.shape[1] == 0:\n",
    "        mfcc_delta2 = np.zeros((13, 1))\n",
    "    \n",
    "    spec = np.abs(librosa.stft(current_chunk, n_fft=n_fft, hop_length=hop_length))\n",
    "\n",
    "    if spec.shape[1] == 0:\n",
    "        spectral_centroid = 0\n",
    "        spectral_flux = 0\n",
    "    else:\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(\n",
    "            S=spec, sr=sr).mean()\n",
    "        spectral_flux = np.mean(np.diff(spec, axis=1)**2) if spec.shape[1] > 1 else 0\n",
    "    \n",
    "    zcr = librosa.feature.zero_crossing_rate(\n",
    "        current_chunk, \n",
    "        frame_length=n_fft, \n",
    "        hop_length=hop_length\n",
    "    ).mean()\n",
    "    ste = np.mean(current_chunk**2)\n",
    "    \n",
    "    features = np.concatenate([\n",
    "        np.mean(mfccs, axis=1),         \n",
    "        np.mean(mfcc_delta, axis=1),   \n",
    "        np.mean(mfcc_delta2, axis=1),   \n",
    "        [spectral_centroid, zcr, spectral_flux, ste]  \n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_audio_file(audio_path, sr=16000, \n",
    "                      chunk_duration=0.025, hop_duration=0.01, context_size=5):\n",
    "        \n",
    "    audio, sr = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    \n",
    "    audio = apply_filters(audio, sr)\n",
    "    audio = loudness_normalization(audio)\n",
    "    audio = apply_wiener_filter(audio)\n",
    "    \n",
    "    \n",
    "    chunk_samples = int(chunk_duration * sr)\n",
    "    hop_samples = int(hop_duration * sr)\n",
    "    num_chunks = (len(audio) - chunk_samples) // hop_samples + 1\n",
    "    \n",
    "    feature_dim = 13 * 3 + 4 \n",
    "    all_features = np.zeros((num_chunks, feature_dim))\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        chunk_start = i * hop_samples\n",
    "        chunk_end = chunk_start + chunk_samples\n",
    "        \n",
    "        all_features[i] = extract_features(\n",
    "            audio, sr, chunk_start, chunk_end, context_size\n",
    "        )                            \n",
    "    \n",
    "    feature_names = (\n",
    "        [f\"mfcc_{i+1}\" for i in range(13)] +\n",
    "        [f\"mfcc_delta_{i+1}\" for i in range(13)] +\n",
    "        [f\"mfcc_delta2_{i+1}\" for i in range(13)] +\n",
    "        ['spectral_centroid', 'zcr', 'spectral_flux', 'ste']\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(all_features, columns=feature_names)\n",
    "    df[\"start_time\"] = np.arange(num_chunks) * hop_duration\n",
    "    df[\"end_time\"] = df[\"start_time\"] + chunk_duration\n",
    "    df[\"file_path\"] = audio_path\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_directory(input_dir, output_csv):\n",
    "    all_data = []    \n",
    "    total_files = sum(1 for root, _, files in os.walk(input_dir) \n",
    "                     for f in files if f.endswith('.wav'))\n",
    "    \n",
    "    with tqdm(total=total_files, desc=\"Processing audio files\") as pbar:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    audio_path = os.path.join(root, file)\n",
    "                    \n",
    "                    df = process_audio_file(audio_path)\n",
    "                    all_data.append(df)\n",
    "                    pbar.update(1)\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    return combined_df"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:26:13.942445Z",
     "iopub.status.busy": "2025-02-24T10:26:13.942070Z",
     "iopub.status.idle": "2025-02-24T10:26:13.952731Z",
     "shell.execute_reply": "2025-02-24T10:26:13.951619Z",
     "shell.execute_reply.started": "2025-02-24T10:26:13.942412Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T15:18:41.307406Z",
     "start_time": "2025-03-02T15:18:41.301830Z"
    }
   },
   "source": [
    "def prepare_features(df):\n",
    "        scaler = StandardScaler()\n",
    "        feature_cols = [col for col in df.columns if any(\n",
    "            col.startswith(prefix) for prefix in ['mfcc_', 'spectral_', 'zcr', 'ste']\n",
    "        )]\n",
    "        \n",
    "        X = df[feature_cols].values\n",
    "        # scale for k means\n",
    "        X = scaler.fit_transform(X)\n",
    "        return X\n",
    "     \n",
    "def apply_kmeans(X, n_clusters=2):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    # we take the last column, which is ste (noise ste < speech ste)\n",
    "    cluster_energies = [X[labels == i, -1].mean() for i in range(n_clusters)]\n",
    "    if cluster_energies[0] > cluster_energies[1]:\n",
    "        labels = 1 - labels\n",
    "        \n",
    "    return labels\n",
    "    \n",
    "    \n",
    "def correct_vad_predictions(df, threshold_ms=100):\n",
    "    '''\n",
    "    If a switch between speech and silence lasts for less than 100ms, \n",
    "    it is replaced with the surrounding majority class.\n",
    "    '''\n",
    "    df_copy = df.copy()\n",
    "    min_frames = threshold_ms // 10 \n",
    "    predictions = df_copy['vad_prediction'].values\n",
    "    \n",
    "    changes = np.diff(predictions)\n",
    "    change_points = np.where(changes != 0)[0] + 1\n",
    "    \n",
    "    segments = np.concatenate(([0], change_points, [len(predictions)]))\n",
    "    \n",
    "    for i in range(len(segments) - 1):\n",
    "        segment_length = segments[i + 1] - segments[i]\n",
    "        \n",
    "        if segment_length < min_frames:\n",
    "            value_before = predictions[segments[i] - 1] if segments[i] > 0 else predictions[segments[i]]\n",
    "            value_after = predictions[segments[i + 1]] if segments[i + 1] < len(predictions) else predictions[segments[i + 1] - 1]\n",
    "            \n",
    "            if value_before == value_after:\n",
    "                predictions[segments[i]:segments[i + 1]] = value_before\n",
    "    \n",
    "    df_copy['vad_prediction'] = predictions\n",
    "    \n",
    "    return df_copy"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T16:37:53.399262Z",
     "start_time": "2025-03-02T16:37:52.934002Z"
    }
   },
   "cell_type": "code",
   "source": "silero_model, silero_utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/skashperova/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:26:13.954425Z",
     "iopub.status.busy": "2025-02-24T10:26:13.954028Z",
     "iopub.status.idle": "2025-02-24T10:26:13.977279Z",
     "shell.execute_reply": "2025-02-24T10:26:13.976097Z",
     "shell.execute_reply.started": "2025-02-24T10:26:13.954385Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T16:44:25.427606Z",
     "start_time": "2025-03-02T16:44:25.407537Z"
    }
   },
   "source": [
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.detection import DetectionErrorRate, DetectionPrecision, DetectionRecall, DetectionPrecisionRecallFMeasure\n",
    "\n",
    "\n",
    "def load_silero_vad(audio_path):\n",
    "    wav, sr = librosa.load(audio_path, sr=16000)\n",
    "    (get_speech_timestamps, _, _, _, _) = silero_utils\n",
    "    speech_timestamps = get_speech_timestamps(wav, silero_model, return_seconds=True)\n",
    "    return speech_timestamps\n",
    "\n",
    "\n",
    "def custom_vad_segments(df, frame_duration=0.01):\n",
    "    \"\"\"Get speech segments from custom VAD predictions\"\"\"\n",
    "    predictions = df['vad_prediction'].values\n",
    "    segments = []\n",
    "    changes = np.diff(predictions.astype(int))\n",
    "    change_points = np.where(changes != 0)[0] + 1\n",
    "    \n",
    "    if predictions[0] == 1:\n",
    "        change_points = np.concatenate(([0], change_points))\n",
    "    if predictions[-1] == 1:\n",
    "        change_points = np.concatenate((change_points, [len(predictions)]))\n",
    "    \n",
    "    for i in range(0, len(change_points), 2):\n",
    "        if i + 1 < len(change_points):\n",
    "            start = change_points[i] * frame_duration\n",
    "            end = change_points[i + 1] * frame_duration\n",
    "            segments.append({'start': start, 'end': end})\n",
    "    return segments\n",
    "\n",
    "\n",
    "def annotate_segments(reference_segments, hypothesis_segments):\n",
    "    reference = Annotation()\n",
    "    hypothesis = Annotation()\n",
    "    \n",
    "    for segment in reference_segments:\n",
    "        reference[Segment(segment['start'], segment['end'])] = 'SPEECH'\n",
    "    \n",
    "    for segment in hypothesis_segments:\n",
    "        hypothesis[Segment(segment['start'], segment['end'])] = 'SPEECH'\n",
    "        \n",
    "    return reference, hypothesis\n",
    "    \n",
    "\n",
    "def calculate_der(reference_segments, hypothesis_segments):\n",
    "    reference, hypothesis = annotate_segments(reference_segments, hypothesis_segments)\n",
    "    metric = DetectionErrorRate()\n",
    "    _ = metric(reference, hypothesis)\n",
    "    return abs(metric)\n",
    "\n",
    "\n",
    "def calculate_precision(reference_segments, hypothesis_segments):\n",
    "    reference, hypothesis = annotate_segments(reference_segments, hypothesis_segments)\n",
    "    \n",
    "    metric = DetectionPrecision()\n",
    "    _ = metric(reference, hypothesis)\n",
    "    return abs(metric)\n",
    "\n",
    "\n",
    "def calculate_recall(reference_segments, hypothesis_segments):\n",
    "    reference, hypothesis = annotate_segments(reference_segments, hypothesis_segments)\n",
    "    \n",
    "    metric = DetectionRecall()\n",
    "    _ = metric(reference, hypothesis)\n",
    "    return abs(metric)\n",
    "\n",
    "\n",
    "def calculate_f1(reference_segments, hypothesis_segments):\n",
    "    reference, hypothesis = annotate_segments(reference_segments, hypothesis_segments)\n",
    "    \n",
    "    metric = DetectionPrecisionRecallFMeasure()\n",
    "    _ = metric(reference, hypothesis)\n",
    "    return abs(metric)\n",
    "\n",
    "\n",
    "def main(df):\n",
    "    total_error_rate = 0.0\n",
    "    total_f1 = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_precision = 0.0\n",
    "    file_count = 0\n",
    "    \n",
    "    for file_path in df['file_path'].unique():\n",
    "        file_df = df[df['file_path'] == file_path]\n",
    "        audio_path = file_path\n",
    "        \n",
    "        silero_vad_regions = load_silero_vad(audio_path)\n",
    "        custom_vad_segments_result = custom_vad_segments(file_df)\n",
    "                \n",
    "        error_rate = calculate_der(silero_vad_regions, custom_vad_segments_result)\n",
    "        total_f1 += calculate_f1(silero_vad_regions, custom_vad_segments_result)\n",
    "        total_recall += calculate_recall(silero_vad_regions, custom_vad_segments_result)\n",
    "        total_precision += calculate_precision(silero_vad_regions, custom_vad_segments_result)\n",
    "\n",
    "        total_error_rate += error_rate\n",
    "        file_count += 1\n",
    "            \n",
    "    if file_count > 0:\n",
    "        avg_error_rate = (total_error_rate / file_count) * 100\n",
    "        print(f\"\\nAvg Detection Error Rate: {avg_error_rate:.1f}%\")\n",
    "        avg_f1 = (total_f1 / file_count) * 100\n",
    "        print(f\"\\nAvg Detection F1: {avg_f1:.1f}%\")\n",
    "        avg_recall = (total_recall / file_count) * 100\n",
    "        print(f\"\\nAvg Detection Recall: {avg_recall:.1f}%\")\n",
    "        avg_precision = (total_precision / file_count) * 100\n",
    "        print(f\"\\nAvg Detection Precision: {avg_precision:.1f}%\")\n",
    "        \n",
    "    return avg_error_rate, avg_f1, avg_recall, avg_precision"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:26:13.980023Z",
     "iopub.status.busy": "2025-02-24T10:26:13.979629Z",
     "iopub.status.idle": "2025-02-24T10:39:26.757413Z",
     "shell.execute_reply": "2025-02-24T10:39:26.756524Z",
     "shell.execute_reply.started": "2025-02-24T10:26:13.979983Z"
    },
    "trusted": true
   },
   "source": [
    "global_df = []\n",
    "for i in range(10):\n",
    "    input_directory = f\"../data/VCTK-Corpus/VCTK-Corpus/wav48/p23{i}\"\n",
    "    output_csv_path = f\"output_features_{i}.csv\"\n",
    "    if os.path.exists(output_csv_path):\n",
    "        df = pd.read_csv(output_csv_path)\n",
    "    else:\n",
    "        df = process_directory(input_directory, output_csv_path)\n",
    "        global_df.append(df)\n",
    "\n",
    "global_df = pd.concat(global_df, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T16:29:16.290341Z",
     "start_time": "2025-03-02T16:29:16.286949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-24T10:39:26.759424Z",
     "iopub.status.busy": "2025-02-24T10:39:26.759040Z",
     "iopub.status.idle": "2025-02-24T10:39:28.193686Z",
     "shell.execute_reply": "2025-02-24T10:39:28.192827Z",
     "shell.execute_reply.started": "2025-02-24T10:39:26.759386Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-02T16:29:20.990076Z",
     "start_time": "2025-03-02T16:29:18.516910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = prepare_features(global_df)\n",
    "predictions = apply_kmeans(features)\n",
    "global_df[\"vad_prediction\"] = predictions\n",
    "global_df = correct_vad_predictions(global_df)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T16:47:44.806612Z",
     "start_time": "2025-03-02T16:44:33.501306Z"
    }
   },
   "cell_type": "code",
   "source": "avg_error_rate, avg_f1, avg_recall, avg_precision = main(global_df)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Detection Error Rate: 35.3%\n",
      "\n",
      "Avg Detection F1: 76.8%\n",
      "\n",
      "Avg Detection Recall: 67.6%\n",
      "\n",
      "Avg Detection Precision: 97.2%\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 212383,
     "sourceId": 847347,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
