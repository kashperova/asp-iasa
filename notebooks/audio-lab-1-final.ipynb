{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":847347,"sourceType":"datasetVersion","datasetId":212383}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport IPython.display as ipd\nimport librosa\nfrom glob import glob\nimport soundfile as sf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import signal, linalg\nimport sklearn\nfrom ipywidgets import interact\nimport urllib\n\nimport torch\nfrom pprint import pprint\n\nfrom sklearn import preprocessing\nfrom sklearn import cluster\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom tqdm import tqdm\nimport numpy as np\nimport soundfile as sf\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport scipy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T17:03:58.240642Z","iopub.execute_input":"2025-02-23T17:03:58.241078Z","iopub.status.idle":"2025-02-23T17:03:59.136395Z","shell.execute_reply.started":"2025-02-23T17:03:58.241045Z","shell.execute_reply":"2025-02-23T17:03:59.135180Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def apply_filters(audio, sr):\n    '''\n    apply high-pass filter to filter out non-speech noise < 60Hz\n    '''\n    hp_cutoff = 60\n    Wn_hp = hp_cutoff / (sr / 2)\n    \n    b, a = signal.butter(4, Wn_hp, btype='highpass')\n    audio = signal.filtfilt(b, a, audio)\n\n    return audio\n    \ndef loudness_normalization(audio):\n    \n    return librosa.util.normalize(audio)\n\ndef apply_wiener_filter(y, sr=16000, win_length=7, hop_length=512):\n    '''Noise filter'''\n    filtered_audio = wiener(y, mysize=win_length)\n    return filtered_audio\n\ndef extract_features(audio, sr, chunk_start, chunk_end, context_size=5):\n    '''\n    Extracting features in current chunk and in context window of n chunks\n    '''\n    current_chunk = audio[chunk_start:chunk_end]\n    \n    context_samples = int(context_size * (chunk_end - chunk_start))\n    # might need a better context calculation (pad with something like zeros or mean values)\n    left_start = max(0, chunk_start - context_samples)\n    right_end = min(len(audio), chunk_end + context_samples)\n    \n    left_context = audio[left_start:chunk_start]\n    right_context = audio[chunk_end:right_end]\n    \n    full_context = np.concatenate([left_context, current_chunk, right_context])\n    \n    n_fft = min(512, len(current_chunk))\n    hop_length = n_fft // 4\n    \n    mfccs = librosa.feature.mfcc(\n        y=current_chunk, \n        sr=sr, \n        n_mfcc=13,\n        n_fft=n_fft,\n        hop_length=hop_length\n    )\n    \n    mfccs_with_context = librosa.feature.mfcc(\n        y=full_context, \n        sr=sr, \n        n_mfcc=13,\n        n_fft=n_fft,\n        hop_length=hop_length\n    )\n    # delta features need context window, they track how the mfccs change over time \n    mfcc_delta = librosa.feature.delta(mfccs_with_context)\n    mfcc_delta2 = librosa.feature.delta(mfccs_with_context, order=2)\n    \n    if mfccs.shape[1] == 0:\n        mfccs = np.zeros((13, 1))\n    if mfcc_delta.shape[1] == 0:\n        mfcc_delta = np.zeros((13, 1))\n    if mfcc_delta2.shape[1] == 0:\n        mfcc_delta2 = np.zeros((13, 1))\n    \n    spec = np.abs(librosa.stft(current_chunk, n_fft=n_fft, hop_length=hop_length))\n\n\n    # classical features (we need to add pitch or something realated!!!)\n    if spec.shape[1] == 0:\n        spectral_centroid = 0\n        spectral_flux = 0\n    else:\n        spectral_centroid = librosa.feature.spectral_centroid(\n            S=spec, sr=sr).mean()\n        spectral_flux = np.mean(np.diff(spec, axis=1)**2) if spec.shape[1] > 1 else 0\n    \n    zcr = librosa.feature.zero_crossing_rate(\n        current_chunk, \n        frame_length=n_fft, \n        hop_length=hop_length\n    ).mean()\n    ste = np.mean(current_chunk**2)\n    \n    features = np.concatenate([\n        np.mean(mfccs, axis=1),         \n        np.mean(mfcc_delta, axis=1),   \n        np.mean(mfcc_delta2, axis=1),   \n        [spectral_centroid, zcr, spectral_flux, ste]  \n    ])\n    \n    return features\n\ndef process_audio_file(audio_path, get_speech_timestamps, model, sr=16000, \n                      chunk_duration=0.025, hop_duration=0.01, context_size=5):\n        \n    audio, sr = librosa.load(audio_path, sr=sr, mono=True)\n    \n    # filter out < 60Hz no speech for sure (Might need to change to 80Hz, but evidence needed, \n    #like a spectrogram and audio before/after)\n    \n    audio = apply_filters(audio, sr)\n    audio = loudness_normalization(audio)\n    # noise reduce\n    # evidence needed! Tune it\n    audio = apply_wiener_filter(audio)\n    \n    \n    chunk_samples = int(chunk_duration * sr)\n    hop_samples = int(hop_duration * sr)\n    num_chunks = (len(audio) - chunk_samples) // hop_samples + 1\n    \n    feature_dim = 13 * 3 + 4 \n    all_features = np.zeros((num_chunks, feature_dim))\n    labels = np.zeros(num_chunks, dtype=int)\n    \n    for i in range(num_chunks):\n        chunk_start = i * hop_samples\n        chunk_end = chunk_start + chunk_samples\n        \n        all_features[i] = extract_features(\n            audio, sr, chunk_start, chunk_end, context_size\n        )\n        \n        chunk_start_sample = chunk_start\n        chunk_end_sample = chunk_end\n                            \n    \n    feature_names = (\n        [f\"mfcc_{i+1}\" for i in range(13)] +\n        [f\"mfcc_delta_{i+1}\" for i in range(13)] +\n        [f\"mfcc_delta2_{i+1}\" for i in range(13)] +\n        ['spectral_centroid', 'zcr', 'spectral_flux', 'ste']\n    )\n    \n    df = pd.DataFrame(all_features, columns=feature_names)\n    df[\"start_time\"] = np.arange(num_chunks) * hop_duration\n    df[\"end_time\"] = df[\"start_time\"] + chunk_duration\n    df[\"file_path\"] = audio_path\n    \n    return df\n\n\ndef process_directory(input_dir, output_csv, get_speech_timestamps, model, \n                     save_interval=100):\n    all_data = []\n    processed_count = 0\n    error_count = 0\n    \n    total_files = sum(1 for root, _, files in os.walk(input_dir) \n                     for f in files if f.endswith('.wav'))\n    \n    with tqdm(total=total_files, desc=\"Processing audio files\") as pbar:\n        for root, _, files in os.walk(input_dir):\n            for file in files:\n                if file.endswith(\".wav\"):\n                    audio_path = os.path.join(root, file)\n                    \n                    df = process_audio_file(audio_path, get_speech_timestamps, model)\n                    all_data.append(df\n                    pbar.update(1)\n    \n    if all_data:\n        combined_df = pd.concat(all_data, ignore_index=True)\n        combined_df.to_csv(output_csv, index=False)\n    \n    return combined_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" def prepare_features(df):\n        scaler = StandardScaler()\n        feature_cols = [col for col in df.columns if any(\n            col.startswith(prefix) for prefix in ['mfcc_', 'spectral_', 'zcr', 'ste']\n        )]\n        \n        X = df[feature_cols].values\n        # scale for k means\n        X = scaler.fit_transform(X)\n        return X\n     \ndef apply_kmeans(X, n_clusters=2):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(X)\n    # we take the last column, which is ste (noiste ste < speech ste)\n    cluster_energies = [X[labels == i, -1].mean() for i in range(n_clusters)]\n    if cluster_energies[0] > cluster_energies[1]:\n        labels = 1 - labels\n        \n    return labels\n    \ndef correct_vad_predictions(df, threshold_ms=100):\n    '''\n    If a switch between speech and silence lasts for less than 100ms, \n    it is replaced with the surrounding majority class.\n    '''\n    df_copy = df.copy()\n    min_frames = threshold_ms // 10 \n    predictions = df_copy['vad_prediction'].values\n    \n    changes = np.diff(predictions)\n    change_points = np.where(changes != 0)[0] + 1\n    \n    segments = np.concatenate(([0], change_points, [len(predictions)]))\n    \n    for i in range(len(segments) - 1):\n        segment_length = segments[i + 1] - segments[i]\n        \n        if segment_length < min_frames:\n            value_before = predictions[segments[i] - 1] if segments[i] > 0 else predictions[segments[i]]\n            value_after = predictions[segments[i + 1]] if segments[i + 1] < len(predictions) else predictions[segments[i + 1] - 1]\n            \n            if value_before == value_after:\n                predictions[segments[i]:segments[i + 1]] = value_before\n    \n    df_copy['vad_prediction'] = predictions\n    \n    return df_copy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyannote.core import Annotation, Segment\nfrom pyannote.metrics.detection import DetectionErrorRate\n\ndef load_silero_vad(audio_path):\n    wav, sr = librosa.load(audio_path, sr=16000)\n    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n    (get_speech_timestamps, _, _, _, _) = utils\n    speech_timestamps = get_speech_timestamps(wav, model, return_seconds=True)\n    return speech_timestamps\n\ndef custom_vad_segments(df, frame_duration=0.01):\n    \"\"\"Get speech segments from custom VAD predictions\"\"\"\n    predictions = df['vad_prediction'].values\n    segments = []\n    changes = np.diff(predictions.astype(int))\n    change_points = np.where(changes != 0)[0] + 1\n    \n    if predictions[0] == 1:\n        change_points = np.concatenate(([0], change_points))\n    if predictions[-1] == 1:\n        change_points = np.concatenate((change_points, [len(predictions)]))\n    \n    for i in range(0, len(change_points), 2):\n        if i + 1 < len(change_points):\n            start = change_points[i] * frame_duration\n            end = change_points[i + 1] * frame_duration\n            segments.append({'start': start, 'end': end})\n    return segments\n\ndef calculate_der(reference_segments, hypothesis_segments):\n    reference = Annotation()\n    hypothesis = Annotation()\n    \n    for segment in reference_segments:\n        reference[Segment(segment['start'], segment['end'])] = 'SPEECH'\n    \n    for segment in hypothesis_segments:\n        hypothesis[Segment(segment['start'], segment['end'])] = 'SPEECH'\n    \n    metric = DetectionErrorRate()\n    _ = metric(reference, hypothesis)\n    return abs(metric)\n\ndef main(df, audio_directory):\n    total_error_rate = 0\n    file_count = 0\n    \n    for file_path in df['file_path'].unique():\n        file_df = df[df['file_path'] == file_path]\n        audio_path = os.path.join(audio_directory, file_path)\n        \n        silero_vad_regions = load_silero_vad(audio_path)\n        custom_vad_segments_result = custom_vad_segments(file_df)\n        \n        # print(\"Silero VAD Results:\")\n        # for region in silero_vad_regions:\n        #     print(f\"Start: {region['start']:.3f}, End: {region['end']:.3f}\")\n        \n        # print(\"\\nCustom VAD Results:\")\n        # for segment in custom_vad_segments_result:\n        #     print(f\"Start: {segment['start']:.3f}, End: {segment['end']:.3f}\")\n        \n        error_rate = calculate_der(silero_vad_regions, custom_vad_segments_result)\n        \n        total_error_rate += error_rate\n        file_count += 1\n    \n    if file_count > 0:\n        average_error_rate = (total_error_rate / file_count) * 100\n        print(f\"\\nAverage Detection Error Rate across all files: {average_error_rate:.1f}%\")\n    \n    return average_error_rate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_directory = \"/kaggle/input/english-multispeaker-corpus-for-voice-cloning/VCTK-Corpus/VCTK-Corpus/wav48/p225\"\noutput_csv_path = \"output_features.csv\"\nget_speech_timestamps, model, read_audio = load_vad_model()\ndf = process_directory(input_directory, output_csv_path, get_speech_timestamps, model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio_directory = '/kaggle/input/english-multispeaker-corpus-for-voice-cloning/VCTK-Corpus/VCTK-Corpus/wav48/p226'\nmain(df, audio_directory)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}